{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2 | Big Data Management & Analysis\n",
    "### Nina Nurrahmawati\n",
    "nn1221@nyu.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172-16-46-16.dynapool.nyu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test spark\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "Your task is to compute the number of restaurants per each cuisine using the provided nyc_restaurant_inspections.csv data set. The CSV file is available for download under the Data Sets page on NYU Classes. The cuisine type can be extracted from the \"CUISINE DESCRIPTION\" column. Note that, a single restaurant may be inspected multiple times. You must use the \"CAMIS\" column to keep only unique restaurants while computing the number of restaurants per cuisine.\n",
    "\n",
    "You MUST turn in a Spark notebook, AND a Spark script that works on the NYU Dumbo cluster. For your script, the input and out file path should be specified through the command line. Basically, I should be able to run your script as:\n",
    "\n",
    "spark-submit YOUR_SCRIPT.py nyc_restaurant_inspections.csv output_folder\n",
    "\n",
    "and it will produce an output on HDFS under output_folder.\n",
    "\n",
    "Using \"hadoop fs -getmerge\" or through the notebook, I should get the expected output similar to the below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('American', 6002),\n",
    " ('Chinese', 2399),\n",
    " ('Cafe', 1629),\n",
    " ('Other', 1296),\n",
    " ('Pizza', 1186),\n",
    " ('Italian', 1016),\n",
    " ('Mexican', 877),\n",
    " ('Japanese', 859),\n",
    " ('Latin (Cuban, Dominican, Puerto Rican, South & Central American)', 840),\n",
    " ('Bakery', 733),\n",
    " ('Caribbean', 671),\n",
    " ('Spanish', 644),\n",
    " ('Donuts', 537),\n",
    " ('Pizza/Italian', 483),\n",
    " ('Chicken', 456),\n",
    " ('Sandwiches', 406),\n",
    " ('Juice, Smoothies, Fruit Salads', 382),\n",
    " ('Hamburgers', 378),\n",
    " ('Asian', 371),\n",
    " ('Ice Cream, Gelato, Yogurt, Ices', 339),\n",
    " ('Indian', 332),\n",
    " ('Jewish/Kosher', 327),\n",
    " ('French', 319),\n",
    " ('Delicatessen', 294),\n",
    " ('Thai', 286)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "filename = 'nyc_restaurant_inspections.csv'\n",
    "rdd = sc.textFile(filename, use_unicode=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'CAMIS'),\n",
       " (1, 'DBA'),\n",
       " (2, 'BORO'),\n",
       " (3, 'BUILDING'),\n",
       " (4, 'STREET'),\n",
       " (5, 'ZIPCODE'),\n",
       " (6, 'PHONE'),\n",
       " (7, 'CUISINE DESCRIPTION'),\n",
       " (8, 'INSPECTION DATE'),\n",
       " (9, 'ACTION'),\n",
       " (10, 'VIOLATION CODE'),\n",
       " (11, 'VIOLATION DESCRIPTION'),\n",
       " (12, 'CRITICAL FLAG'),\n",
       " (13, 'SCORE'),\n",
       " (14, 'GRADE'),\n",
       " (15, 'GRADE DATE'),\n",
       " (16, 'RECORD DATE'),\n",
       " (17, 'INSPECTION TYPE')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check column index\n",
    "list(enumerate(rdd.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter (cuisine type, camis id)\n",
    "def extractCuisine(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        next(list_of_records) # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        (camis, cuisine) = (row[0], row[7])\n",
    "        yield (camis, cuisine)\n",
    "# take unique value, result => (camis_id, cuisine_type)\n",
    "cuisine = rdd.mapPartitionsWithIndex(extractCuisine).distinct()\n",
    "# count the cuisine\n",
    "counts = cuisine.map(lambda x: (x[1], 1)) \\\n",
    "        .reduceByKey(lambda x,y: x+y) \\\n",
    "        .top(25, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('American', 6002),\n",
       " ('Chinese', 2399),\n",
       " ('CafÃ©/Coffee/Tea', 1629),\n",
       " ('Other', 1296),\n",
       " ('Pizza', 1186),\n",
       " ('Italian', 1016),\n",
       " ('Mexican', 877),\n",
       " ('Japanese', 859),\n",
       " ('Latin (Cuban, Dominican, Puerto Rican, South & Central American)', 840),\n",
       " ('Bakery', 733),\n",
       " ('Caribbean', 671),\n",
       " ('Spanish', 644),\n",
       " ('Donuts', 537),\n",
       " ('Pizza/Italian', 483),\n",
       " ('Chicken', 456),\n",
       " ('Sandwiches', 406),\n",
       " ('Juice, Smoothies, Fruit Salads', 382),\n",
       " ('Hamburgers', 378),\n",
       " ('Asian', 371),\n",
       " ('Ice Cream, Gelato, Yogurt, Ices', 339),\n",
       " ('Indian', 332),\n",
       " ('Jewish/Kosher', 327),\n",
       " ('French', 319),\n",
       " ('Delicatessen', 294),\n",
       " ('Thai', 286)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd.mapPartitionsWithIndex(extractCuisine).distinct().map(lambda x: (x[1], 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y))\n",
    "#     .top(25, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
